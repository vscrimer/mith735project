---
layout: page
title: About
permalink: /about/
---
<div>
  <p>
    <h1>Why are We Doing This? An Environmental Scan</h1>
    <p>Bibliometrics is not a new approach to evaluating written material particularly in the sciences.
    Eugene Garfield created the science Citation Index (SCI) in 1964. The SCI was an experiment in the
    automated indexing of bibliographic citations designed to help scientific researchers more quickly
    and efficiently find new research being published in related fields. An expanded version of this
    original project survives today as the subscription-based, <a href="https://clarivate.com/webofsciencegroup/solutions/web-of-science/">
      Web of Science</a> owned by Clarivate Analytics.
    As of 2014, Web of Science had expanded its bibliographic index to include over 1,700 arts and humanities
     journals. This tool allows me, for instance, to search for the most cited articles from a scholarly
     publication like Theatre Journal, to see where they have been cited and by whom.
    It also allows users to generate a comprehensive citation report for an author, journal, or article that
     shows the number of publications and sum of times those publications have been cited over time.
     This citation report can then be exported to an excel or text file for further imaging and analysis.
      I could, for example, with this information then create data visualizations comparing how often two different journals are cited in any given year. Web of Science’s citation report also offers an h-index rating which is a metric for measuring the value of a published work based on the number of
    articles it produces (citations) and the citation impact of those subsequent articles.</p>
  <p>The inclusion of the h-index rating seems to reflect what has become the primary purpose of citation
  analysis
  which is to use citations as an indicator of scholarly validity or quality. The underlying assumption of
  this type of citation analysis is that highly cited works which spawn other influential
  (read: highly cited) works are examples of scholarly excellence in the field. A citation report for
  Theatre Journal does, as first blush, seem to return results that accurately reflect what I know to
  be the prominent authors and subjects in my field. But the underlying assumption regarding the importance
   of citations in determining scholarly quality becomes somewhat self-fulfilling with the advent and increased
  use of citation indexes which efficiently and in greater numbers direct researchers and scholars to highly
  cited (presumably excellent) articles which these scholars will then inevitably cite, exponentially
  perpetuating the prominence of the highest ranked articles (Aksnes 2003). While scholarly research should,
  of course, take part in existing discourse across any given field, using bibliometrics as a measure of
  success for faculty, universities, journal editors, grant applicants, and job-seekers tends to favor
  already established scholars and institutions (Merton 1968) and does not necessarily
  reflect the quality of or reasons for engagement with a particular work being cited (MacRoberts 1996).</p>

  <p>Furthermore, most citation indexes offer poor coverage of work in the arts and humanities relative to
    coverage of scientific publications and may, therefore, offer skewed results when assessing citations
     in the social sciences and humanities (Waltman 2016). While Google Scholar (a freely accessible
     bibliographic database similar to Web of Science) does an arguably better job cataloging a broader
     range of literature in the humanities its search algorithms weight citation rankings heavily when
     returning top results, intensifying the issue of scholars reiteratively citing already established
     scholars and well-known articles.</p>
  <p>With these limitations in mind and conscious of the potential for bibliometrics to become a tool of
  hegemony, my project proposes an inversion or adaptation to the typical citation analysis. Rather than
  using citation analysis to search for the most heavily cited articles within a discipline and then using
   that information to further validate a scholar or journal’s existing standing in the field, I am
   interested in using citation analysis to paint a landscape of who and what is being cited within the
   featured articles in major journals in the field. While typical citation analysis like those
  facilitated by Web of Science or Google Scholar begin a search with an author or publication and search
  outward for citations, creating a networked web of citations flowing outward from a central node,
  I am interested in a survey of which authors and which types of materials are cited across time
  within a journal itself. Typical citation analysis asks, <font color="red">“Who is citing us out there?”</font> whereas this
  project asks, <font color="blue">“Who are we citing in here?”</font> This type of citation analysis, because it faces inward rather
  than externally, focuses not on measuring value to a broader field but asks critical questions about our
  citational behavior within that field.</p>

  <p>At its most basic level, a citation analysis across a range of scholarly journals, such as the one
  I have proposed, constitutes a quantitative approach to visualizing the habits adopted within a
  particular field of study rather than an evaluation of a particular article or journal’s influence
  or efficacy. In this, my project is not unique. Particularly within the digital humanities, scholars
  have employed quantitative or mixed-method analyses of the text of journals, conference proceedings,
  professional organization membership lists, and anthologies to illustrate patterns in everything from
  geography, race, gender, and subject matter. These analyses are then usually used to either confirm or deny
  assumptions about the proclivities of a given discipline and sometimes to make recommendations.
  In general,
  these projects, while using digital tools to analyze a digital corpus, seek a fairly static end in that the final product is designed to live not on a website but as fairly traditional academic conference papers or journal articles. These papers can, of course, be accessed online but do not necessarily allow for user engagement or fluid updating in the same way housing the project on a website would. That being said, some of the results of these projects circulate via author’s blogs or other open access sites like ResearchGate.  The following is a brief overview of several related projects that use
  quantitative methods to analyze a large corpus of text in order draw broad conclusions about typical
  practices within an academic field.</p>
  <p>For instance, <a href="http://scottbot.net/wp-content/uploads/2011/11/weingart-Gender-Centrality.pdf">
     Nickoal Eichmann, Jeana Jorgensen, Scott B. Weingart’s 2016 project </a> tracks
    representation at DH conferences between 2000 and 2015 (Weingart et al. 2016). Rather than
    taking a corpus of journal articles as its subject, for this study, the authors gathered the publicly
    available conference programs for each year of the Digital Humanities conference organized by the
    Alliance of Digital Humanities Organizations (ADHO) and manually entered information in a spreadsheet,
    tracking the title and keywords from accepted papers as well as the author’s name, gender, geographic
    location, and institutional association. While the author’s themselves admit the limitations of making
     binary gender assignations based on the names of the presenters in their dataset, they suggest names
      serve as an imperfect but useful proxy for gender as immediately perceived by others within the
      field. The manual collection of data modeled in this project is the most direct (and likely the most
       time-consuming) method for obtaining and organizing a standardized list of bibliographic information
       from a fairly large body of text, but this project only logged 3,239 unique authors across DH
       conferences over the last fifteen years. For a project like the one I propose, in which the number
        of unique bibliographic sources would likely number in the tens of thousands, manual data entry
        would be too time-consuming for the number of researchers available. </p>

<p>  In a similar vein, Julie Nyhan and Oliver Duke-Williams published an article in Literary and Linguistic
  Computing (2014) which shares the results of a statistical analysis of collaborative publishing patterns
  across three digital humanities journals, the results of which effectively debunk assumptions about the
  collaborative nature of digital humanities scholarship (Nyhan 2014). The authors of the study used Zotero
  to extract bibliographical metadata from the three journals they analyzed (as well as one journal outside
  their field for comparative purposes). They then exported their data to excel where it was cleaned up and
   regularized. As I have proposed in my own project, the authors here excluded reviews, letters to the
   editor, and other non-peer reviewed notes to avoid skewing data and they noted that Zotero was not an
   ideal tool for differentiating between research papers and other content. Unlike the project I have
   proposed, Nyhan et al. are gathering bibliographic metadata only on the author of each article within
   a journal rather than bibliographic data on all the authors cited within that article. Zotero only
   provides bibliographic metadata for the author of the article itself, so it would not be the
   idea tool for our project, but this project does bring us a step closer to understanding how digital
    tools might help expedite the work of collecting and parsing bibliographic information from a body of
     digital texts.</p>
<p>Students of Duke-Williams, Gao et al. presented their project
  <a href= "https://dh2018.adho.org/en/visualising-the-digital-humanities-community-a-comparison-study-between-citation-network-and-social-network/">
     “Visualizing the Digital Humanities Community:
 A Comparison Study Between Citation Network and Social Network”</a> at the 2018 ADHO conference in Mexico City.
  This project extends the work of Nyhan and Duke-Williams but expands the citation analysis to Twitter
  users followed by ADHO and its member organizations. Researchers calculated the number of non-self retweets a user
  receives and co-retweet counts (or the number of same tweets that two users both retweeted) to build
  network graphs that show citation clusters by topic, language, and region. </p>
<p>A Northwestern doctoral student, Gregory Palermo’s
  <a href="https://palermog.github.io/blog/2017/02/DHQ-Aggregate.html">
    citation analysis of DH Quarterly (2019)
    </a> is the first
citation analysis project I have seen that analyses the bibliographic information from the backmatter of
each article within a journal rather than just the author of the article itself. He even plots
the publication dates of the cited works instead of the articles, answering from where and when are
 DHQ authors citing. This is very similar to the type of project we want to undertake, and Palermo
  provides the R code he uses for this project on his blog. Thus far we had been thinking of using
  the AnyStyle parser to help create a dataset from our corpus of texts, but it is possible that we
  could use Palermo’s code to perform the type of citation analysis we are hoping to do. </p>
<p>As is obvious in the initial findings of this environmental scan, a majority of citation analysis of
 the type we have proposed are undertaken in the digital humanities. I have yet to come across a citation
 analysis of journals in theatre or performance studies, indicating a critical opening in the field to
 which our project might contribute. </p>

<p><b><font color="purple"> WORKS CITED </font></b></p>

<p>  Aksnes, D. W., Langfeldt, L., & Wouters, P. (2019). Citations, Citation Indicators, and Research Quality: An Overview of Basic Concepts and Theories. SAGE Open, 9(1).</p>

<p>  Aksnes, D. W. (2003). Characteristics of highly cited papers. Research Evaluation, 12, 159-170.</p>

<p>  de Rijcke, S., Wouters, P. F., Rushforth, A. D., Franssen, T. P., Hammarfelt, B. (2016). Evaluation practices and effects of indicator use—A literature review. Research Evaluation, 25, 161-169.</p>

<p>  Gao, J., Duke-Williams, O., & Mahony, S. (n.d.). The Intellectual Structure of Digital Humanities: An Author Co-Citation Analysis. 3.</p>

<p>  Merton, R. K. (1968). The Matthew effect in science. Science, 159, 56-63.</p>

<p>  MacRoberts, M. H., MacRoberts, B. R. (1996). Problems of citation analysis. Scientometrics, 36, 435-444.</p>

  <p>Nyhan, J., & Duke-Williams, O. (2014). Joint and multi-authored publication patterns in the Digital Humanities. Literary and Linguistic Computing, 29(3), 387–399.</p>

  <p>Waltman, L. (2016). A review of the literature on citation impact indicators. Journal of Informetrics, 10, 365-391.</p>

  <p>Weingart, Scott B. (2016, March 22). Representation at Digital Humanities Conferences (2000-2015). Retrieved November 12, 2019, from The scottbot irregular website: http://scottbot.net/representation-at-digital-humanities-conferences-2000-2015/
  </p>


</div>
